# -*- coding: utf-8 -*-
"""Feature Engineering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tCXarDl5yynZxKWbRybCTdIkOl8OjkaJ

# **`*`Handling_Missing_values`*`**
"""

import seaborn as sns

df=sns.load_dataset('titanic')

df.head()

## Check missing values
df.isnull().sum()

## Delete the rows or data point to handle missing values

df.shape

df.dropna().shape

## Column wise deletion
df.dropna(axis=1)

"""## Imputation Missing Values
### 1- Mean Value Imputation
"""

sns.histplot(df['age'],kde=True)

df['Age_mean']=df['age'].fillna(df['age'].mean())

df[['Age_mean','age']]

## MEan Imputation Works Well when we have normally distributed data

"""### 2. Median Value Imputation- If we have outliers in the dataset"""

df['age_median']=df['age'].fillna(df['age'].median())

df[['age_median','Age_mean','age']]

"""### 3. Mode Imputation Technqiue--Categorical values"""

df[df['embarked'].isnull()]

df['embarked'].unique()

mode_value=df[df['embarked'].notna()]['embarked'].mode()[0]

df['embarked_mode']=df['embarked'].fillna(mode_value)

df[['embarked_mode','embarked']]

df['embarked_mode'].isnull().sum()

df['embarked'].isnull().sum()







"""## Handling Imbalanced Dataset

1. Up Sampling
2. Down Sampling
"""

import numpy as np
import pandas as pd

# Set the random seed for reproducibility
np.random.seed(123)

# Create a dataframe with two classes
n_samples = 1000
class_0_ratio = 0.9
n_class_0 = int(n_samples * class_0_ratio)
n_class_1 = n_samples - n_class_0

n_class_0,n_class_1

## CREATE MY DATAFRAME WITH IMBALANCED DATASET
class_0 = pd.DataFrame({
    'feature_1': np.random.normal(loc=0, scale=1, size=n_class_0),
    'feature_2': np.random.normal(loc=0, scale=1, size=n_class_0),
    'target': [0] * n_class_0
})

class_1 = pd.DataFrame({
    'feature_1': np.random.normal(loc=2, scale=1, size=n_class_1),
    'feature_2': np.random.normal(loc=2, scale=1, size=n_class_1),
    'target': [1] * n_class_1
})

df=pd.concat([class_0,class_1]).reset_index(drop=True)

df.tail()

df['target'].value_counts()

## upsampling
df_minority=df[df['target']==1]
df_majority=df[df['target']==0]

from sklearn.utils import resample
df_minority_upsampled=resample(df_minority,replace=True, #Sample With replacement
         n_samples=len(df_majority),
         random_state=42
        )

df_minority_upsampled.shape

df_minority_upsampled.head()

df_upsampled=pd.concat([df_majority,df_minority_upsampled])

df_upsampled['target'].value_counts()

"""## Down Sampling"""

import pandas as pd

# Set the random seed for reproducibility
np.random.seed(123)

# Create a dataframe with two classes
n_samples = 1000
class_0_ratio = 0.9
n_class_0 = int(n_samples * class_0_ratio)
n_class_1 = n_samples - n_class_0

class_0 = pd.DataFrame({
    'feature_1': np.random.normal(loc=0, scale=1, size=n_class_0),
    'feature_2': np.random.normal(loc=0, scale=1, size=n_class_0),
    'target': [0] * n_class_0
})

class_1 = pd.DataFrame({
    'feature_1': np.random.normal(loc=2, scale=1, size=n_class_1),
    'feature_2': np.random.normal(loc=2, scale=1, size=n_class_1),
    'target': [1] * n_class_1
})

df = pd.concat([class_0, class_1]).reset_index(drop=True)

# Check the class distribution
print(df['target'].value_counts())

## downsampling
df_minority=df[df['target']==1]
df_majority=df[df['target']==0]

from sklearn.utils import resample
df_majority_upsampled=resample(df_minority,replace=True, #Sample With replacement
         n_samples=len(df_majority),
         random_state=42
        )







"""## SMOTE(Synthetic Minority Oversampling Technique)
SMOTE (Synthetic Minority Over-sampling Technique) is a technique used in machine learning to address imbalanced datasets where the minority class has significantly fewer instances than the majority class. SMOTE involves generating synthetic instances of the minority class by interpolating between existing instances.
"""

from sklearn.datasets import make_classification

X,y=make_classification(n_samples=1000,n_redundant=0,n_features=2,n_clusters_per_class=1,
                   weights=[0.90],random_state=12)

import pandas as pd
df1=pd.DataFrame(X,columns=['f1','f2'])
df2=pd.DataFrame(y,columns=['target'])
final_df=pd.concat([df1,df2],axis=1)
final_df.head()

final_df['target'].value_counts()

import matplotlib.pyplot as plt
plt.scatter(final_df['f1'],final_df['f2'],c=final_df['target'])

!pip install imblearn

from imblearn.over_sampling import SMOTE

## transform the dataset
oversample=SMOTE()
X,y=oversample.fit_resample(final_df[['f1','f2']],final_df['target'])

X.shape

y.shape

len(y[y==0])

len(y[y==1])

df1=pd.DataFrame(X,columns=['f1','f2'])
df2=pd.DataFrame(y,columns=['target'])
oversample_df=pd.concat([df1,df2],axis=1)

plt.scatter(oversample_df['f1'],oversample_df['f2'],c=oversample_df['target'])





"""# ***`Handling Outlier`***"""



"""## 5 number Summary And Box Plot"""

## Minimum,MAximum,Median,Q1,Q3,IQR

import numpy as np

lst_marks=[45,32,56,75,89,54,32,89,90,87,67,54,45,98,99,67,74]
minimum,Q1,median,Q3,maximum=np.quantile(lst_marks,[0,0.25,0.50,0.75,1.0])

minimum,Q1,median,Q3,maximum

IQR=Q3-Q1
print(IQR)

lower_fence=Q1-1.5*(IQR)
higher_fence=Q3+1.5*(IQR)

lower_fence

higher_fence

lst_marks=[45,32,56,75,89,54,32,89,90,87,67,54,45,98,99,67,74]

import seaborn as sns

sns.boxplot(lst_marks)

lst_marks=[-100,-200,45,32,56,75,89,54,32,89,90,87,67,54,45,98,99,67,74,150,170,180]

sns.boxplot(lst_marks)









"""## Data Encoding

1. Nominal/OHE Encoding
2. Label and Ordinal Encoding
3. Target Guided Ordinal Encoding
"""

import pandas as pd
from sklearn.preprocessing import OneHotEncoder

## Create a simple dataframe
df = pd.DataFrame({
    'color': ['red', 'blue', 'green', 'green', 'red', 'blue']
})

df.head()

##create an instance of Onehotencoder
encoder=OneHotEncoder()

## perform fit and transform
encoded=encoder.fit_transform(df[['color']]).toarray()

import pandas as pd
encoder_df=pd.DataFrame(encoded,columns=encoder.get_feature_names_out())

encoder_df

## for new data
encoder.transform([['blue']]).toarray()

pd.concat([df,encoder_df],axis=1)

import seaborn as sns
sns.load_dataset('tips')

"""### Label Encoding

"""

df.head()

from sklearn.preprocessing import LabelEncoder
lbl_encoder=LabelEncoder()

lbl_encoder.fit_transform(df[['color']])

lbl_encoder.transform([['red']])

lbl_encoder.transform([['blue']])

lbl_encoder.transform([['green']])

"""### Ordinal Encoding

"""

## ORdinal Encoding
from sklearn.preprocessing import OrdinalEncoder

# create a sample dataframe with an ordinal variable
df = pd.DataFrame({
    'size': ['small', 'medium', 'large', 'medium', 'small', 'large']
})

df

## create an instance of ORdinalEncoder and then fit_transform
encoder=OrdinalEncoder(categories=[['small','medium','large']])

encoder.fit_transform(df[['size']])

encoder.transform([['small']])







"""## Target Guided Ordinal Encoding

"""

import pandas as pd

# create a sample dataframe with a categorical variable and a target variable
df = pd.DataFrame({
    'city': ['New York', 'London', 'Paris', 'Tokyo', 'New York', 'Paris'],
    'price': [200, 150, 300, 250, 180, 320]
})

df

mean_price=df.groupby('city')['price'].mean().to_dict()

mean_price

df['city_encoded']=df['city'].map(mean_price)

df[['price','city_encoded']]

import seaborn as sns
sns.load_dataset('tips')







